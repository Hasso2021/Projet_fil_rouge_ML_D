{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Entra√Ænement RL Agent sur Google Colab\n",
        "\n",
        "Ce notebook est **sp√©cifiquement pour entra√Æner l'agent RL** sur Colab avec GPU.\n",
        "Le mod√®le entra√Æn√© sera t√©l√©charg√© pour utilisation locale.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ √âtape 1 : Installation des d√©pendances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation PyTorch avec CUDA 11.8 (pour GPU Colab)\n",
        "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "\n",
        "# D√©pendances principales pour RL\n",
        "%pip install diffusers transformers accelerate -q\n",
        "%pip install stable-baselines3[extra] gymnasium -q\n",
        "%pip install pillow numpy requests pydantic pydantic-settings -q\n",
        "\n",
        "print(\"‚úÖ D√©pendances install√©es\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß √âtape 2 : Cloner et configurer le projet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remplacer par l'URL de votre repository GitHub\n",
        "REPO_URL = \"https://github.com/votre-user/votre-repo.git\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Cloner le repo\n",
        "if not os.path.exists(\"Projet_fil_rouge_ML_DL\"):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "%cd Projet_fil_rouge_ML_DL\n",
        "\n",
        "# Configuration pour Colab (GPU) - Entra√Ænement RL\n",
        "os.environ[\"SD_DEVICE\"] = \"cuda\"\n",
        "os.environ[\"SD_DTYPE\"] = \"float16\"\n",
        "os.environ[\"OUTPUT_DIR\"] = \"outputs\"\n",
        "os.environ[\"MODELS_DIR\"] = \"models\"\n",
        "os.environ[\"RL_AGENT_PATH\"] = \"models/rl_agent.zip\"\n",
        "\n",
        "print(\"‚úÖ Projet configur√© pour entra√Ænement RL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç √âtape 3 : V√©rifier le GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(\"\\n‚ö†Ô∏è IMPORTANT: Assurez-vous d'avoir un GPU activ√© pour l'entra√Ænement RL\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ERREUR: Aucun GPU d√©tect√©. Activez le GPU dans Runtime > Change runtime type\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ √âtape 4 : Entra√Æner l'agent RL\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT :**\n",
        "- Entra√Ænement 10k steps : ~1-2 heures sur GPU\n",
        "- Entra√Ænement 20k steps : ~2-4 heures sur GPU\n",
        "- Le mod√®le sera sauvegard√© dans `models/rl_agent.zip`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration de l'entra√Ænement\n",
        "TOTAL_TIMESTEPS = 10000  # Ajustez selon vos besoins (10000 recommand√© minimum)\n",
        "SAVE_PATH = \"models/rl_agent.zip\"\n",
        "\n",
        "print(f\"üöÄ D√©marrage entra√Ænement RL agent...\")\n",
        "print(f\"üìä Steps d'entra√Ænement: {TOTAL_TIMESTEPS}\")\n",
        "print(f\"‚è±Ô∏è Temps estim√©: {TOTAL_TIMESTEPS // 5000:.1f}-{TOTAL_TIMESTEPS // 2500:.1f} heures\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Importer les modules\n",
        "from app.models.rl_agent import RLOptimizer\n",
        "from training.rl_env import PromptOptimizationEnv\n",
        "\n",
        "# Cr√©er environnement et agent\n",
        "env = PromptOptimizationEnv()\n",
        "agent = RLOptimizer(env=env)\n",
        "\n",
        "# Entra√Æner l'agent\n",
        "agent.train(\n",
        "    total_timesteps=TOTAL_TIMESTEPS,\n",
        "    save_path=SAVE_PATH\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Entra√Ænement termin√© !\")\n",
        "print(f\"üì¶ Mod√®le sauvegard√©: {SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ √âtape 5 : V√©rifier le mod√®le entra√Æn√©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rifier que le mod√®le existe et a la bonne taille\n",
        "import os\n",
        "model_path = \"models/rl_agent.zip\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "    print(f\"‚úÖ Mod√®le trouv√©: {model_path}\")\n",
        "    print(f\"üì¶ Taille: {size_mb:.2f} MB\")\n",
        "    \n",
        "    # Lister tous les fichiers du mod√®le\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(model_path, 'r') as zip_ref:\n",
        "        files = zip_ref.namelist()\n",
        "        print(f\"üìÅ Fichiers dans le mod√®le: {len(files)}\")\n",
        "        for f in files[:5]:  # Afficher les 5 premiers\n",
        "            print(f\"   - {f}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"   ... et {len(files) - 5} autres fichiers\")\n",
        "else:\n",
        "    print(f\"‚ùå ERREUR: Mod√®le non trouv√© √† {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ √âtape 6 : Sauvegarder dans Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monter Google Drive pour sauvegarder le mod√®le\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cr√©er dossier de sauvegarde\n",
        "DRIVE_MODELS_DIR = \"/content/drive/MyDrive/ai-creative-studio/models\"\n",
        "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Copier le mod√®le vers Drive\n",
        "model_name = \"rl_agent.zip\"\n",
        "drive_model_path = f\"{DRIVE_MODELS_DIR}/{model_name}\"\n",
        "\n",
        "!cp models/rl_agent.zip {drive_model_path}\n",
        "\n",
        "print(f\"‚úÖ Mod√®le sauvegard√© dans Google Drive\")\n",
        "print(f\"üì¶ Chemin: {drive_model_path}\")\n",
        "print(f\"\\nüí° Vous pourrez t√©l√©charger depuis Drive ou utiliser sur Colab\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• √âtape 7 : T√©l√©charger le mod√®le localement\n",
        "\n",
        "**Option 1 : T√©l√©chargement direct depuis Colab**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T√©l√©charger le mod√®le directement depuis Colab\n",
        "from google.colab import files\n",
        "\n",
        "model_path = \"models/rl_agent.zip\"\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"üì• T√©l√©chargement de {model_path}...\")\n",
        "    files.download(model_path)\n",
        "    print(\"‚úÖ T√©l√©chargement termin√© !\")\n",
        "    print(\"\\nüí° Placez le fichier dans le dossier 'models/' de votre projet local\")\n",
        "else:\n",
        "    print(f\"‚ùå Mod√®le non trouv√©: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option 2 : T√©l√©charger depuis Google Drive**\n",
        "\n",
        "1. Allez sur [Google Drive](https://drive.google.com)\n",
        "2. Naviguez vers `ai-creative-studio/models/`\n",
        "3. T√©l√©chargez `rl_agent.zip`\n",
        "4. Placez-le dans `models/rl_agent.zip` de votre projet local\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ √âtape 8 : Test rapide du mod√®le (optionnel)\n",
        "\n",
        "Tester que le mod√®le fonctionne correctement avant t√©l√©chargement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test rapide d'optimisation de prompt\n",
        "test_prompt = \"a cat\"\n",
        "\n",
        "print(f\"üß™ Test d'optimisation pour: '{test_prompt}'...\")\n",
        "\n",
        "try:\n",
        "    result = agent.optimize_prompt(\n",
        "        base_prompt=test_prompt,\n",
        "        n_iterations=5  # Test rapide avec 5 it√©rations\n",
        "    )\n",
        "    \n",
        "    print(\"\\nüìä R√©sultats:\")\n",
        "    print(f\"  Prompt original: {result['original_prompt']}\")\n",
        "    print(f\"  Prompt optimis√©: {result['optimized_prompt']}\")\n",
        "    print(f\"  Score original: {result['original_score']:.2f}\")\n",
        "    print(f\"  Score optimis√©: {result['optimized_score']:.2f}\")\n",
        "    print(f\"  Am√©lioration: {result['improvement']:+.2f}\")\n",
        "    print(f\"  Param√®tres: {result['best_params']}\")\n",
        "    print(\"\\n‚úÖ Mod√®le fonctionne correctement !\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur lors du test: {e}\")\n",
        "    print(\"üí° Le mod√®le peut quand m√™me fonctionner localement\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
