import os
from typing import Optional, Dict, Any
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from app.utils.config import settings
from training.rl_env import PromptOptimizationEnv

class RLOptimizer:
    """
    Agent RL qui optimise les prompts pour Stable Diffusion.
    """
    
    def __init__(self, env: Optional[PromptOptimizationEnv] = None):
        self.env = env or PromptOptimizationEnv()
        self.model: Optional[PPO] = None
        
        # Charger modÃ¨le si existe
        if os.path.exists(settings.RL_AGENT_PATH):
            try:
                self.model = PPO.load(settings.RL_AGENT_PATH, env=self.env)
                print(f"âœ… ModÃ¨le RL chargÃ© depuis {settings.RL_AGENT_PATH}")
            except Exception as e:
                print(f"âš ï¸ Erreur lors du chargement du modÃ¨le RL: {e}")
                print(f"ðŸ’¡ VÃ©rifiez que le modÃ¨le existe et est compatible avec stable-baselines3==2.2.1")
                self.model = None
        else:
            print(f"â„¹ï¸ ModÃ¨le RL non trouvÃ© Ã  {settings.RL_AGENT_PATH}")
            print(f"ðŸ’¡ EntraÃ®nez d'abord le modÃ¨le avec training/train_rl_agent.py")
            print(f"   ou tÃ©lÃ©chargez-le depuis Colab (voir WORKFLOW_HYBRIDE.md)")
            self.model = None
    
    def train(self, total_timesteps: int = 10000, save_path: Optional[str] = None):
        """
        EntraÃ®ne l'agent RL.
        
        Args:
            total_timesteps: Nombre total de steps d'entraÃ®nement
            save_path: Chemin pour sauvegarder le modÃ¨le
        """
        if self.model is None:
            # CrÃ©er nouveau modÃ¨le PPO
            self.model = PPO(
                "MlpPolicy",
                self.env,
                learning_rate=3e-4,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,
                verbose=1,
                tensorboard_log="./logs/ppo_prompt_optimizer/"
            )
            print("âœ… Nouveau modÃ¨le PPO crÃ©Ã©")
        
        # Callbacks
        save_path = save_path or settings.RL_AGENT_PATH
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        checkpoint_callback = CheckpointCallback(
            save_freq=1000,
            save_path="./models/checkpoints/",
            name_prefix="ppo_prompt_opt"
        )
        
        print(f"ðŸš€ DÃ©marrage entraÃ®nement PPO ({total_timesteps} steps)...")
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True
        )
        
        # Sauvegarde finale
        self.model.save(save_path)
        print(f"âœ… EntraÃ®nement terminÃ© et modÃ¨le sauvegardÃ© dans {save_path}")
    
    def optimize_prompt(
        self,
        base_prompt: str,
        n_iterations: int = 10
    ) -> Dict[str, Any]:
        """
        Utilise l'agent entraÃ®nÃ© pour optimiser un prompt.
        
        Args:
            base_prompt: Prompt de base Ã  optimiser
            n_iterations: Nombre d'itÃ©rations d'optimisation
        
        Returns:
            dict: RÃ©sultats de l'optimisation
        """
        if self.model is None:
            raise ValueError("ModÃ¨le RL non entraÃ®nÃ©. Appelez train() d'abord.")
        
        # Reset env avec nouveau prompt
        obs, _ = self.env.reset(options={"base_prompt": base_prompt})
        
        # GÃ©nÃ©ration originale (baseline)
        from app.models.stable_diffusion import sd_generator
        from app.models.aesthetic_scorer import aesthetic_scorer
        
        original_image = sd_generator.generate(
            prompt=base_prompt,
            guidance_scale=7.5,
            num_inference_steps=50
        )
        original_score = aesthetic_scorer.score(original_image)
        
        # Optimisation avec agent
        best_prompt = base_prompt
        best_score = original_score
        best_image = original_image
        best_info = {}
        
        for i in range(n_iterations):
            action, _ = self.model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = self.env.step(action)
            
            if reward > best_score:
                best_score = reward
                best_prompt = info['prompt']
                best_info = info
            
            if done or truncated:
                break
        
        # GÃ©nÃ©rer meilleure image avec meilleur prompt
        best_image = sd_generator.generate(
            prompt=best_prompt,
            guidance_scale=best_info.get('guidance_scale', 7.5),
            num_inference_steps=best_info.get('num_steps', 50)
        )
        
        return {
            'original_prompt': base_prompt,
            'optimized_prompt': best_prompt,
            'original_score': float(original_score),
            'optimized_score': float(best_score),
            'improvement': float(best_score - original_score),
            'best_params': {
                'guidance_scale': best_info.get('guidance_scale', 7.5),
                'num_steps': best_info.get('num_steps', 50)
            }
        }

# Instance globale (chargÃ©e Ã  la demande)
rl_optimizer: Optional[RLOptimizer] = None

def get_rl_optimizer() -> RLOptimizer:
    """Retourne l'instance globale de l'optimiseur RL."""
    global rl_optimizer
    if rl_optimizer is None:
        rl_optimizer = RLOptimizer()
    return rl_optimizer

