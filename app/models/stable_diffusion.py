import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from PIL import Image
from typing import Optional
from app.utils.config import settings

class StableDiffusionGenerator:
    def __init__(self):
        self.device = settings.SD_DEVICE
        self.dtype = torch.float16 if settings.SD_DTYPE == "float16" else torch.float32
        
        # Charger le mod√®le
        self.pipe = StableDiffusionPipeline.from_pretrained(
            settings.SD_MODEL_ID,
            dtype=self.dtype,  # Utiliser dtype au lieu de torch_dtype (d√©pr√©ci√©)
            safety_checker=None,
            requires_safety_checker=False
        ).to(self.device)
        
        # Optimiser avec DPM-Solver (g√©rer les configurations diff√©rentes des mod√®les)
        try:
            scheduler_config = self.pipe.scheduler.config.copy()
            
            # Fix pour certains mod√®les qui ont final_sigmas_type="zero"
            # (ex: DreamShaper 8 avec algorithm_type="deis")
            if hasattr(scheduler_config, 'final_sigmas_type'):
                if scheduler_config.get('final_sigmas_type') == 'zero' and \
                   scheduler_config.get('algorithm_type') == 'deis':
                    scheduler_config['final_sigmas_type'] = 'sigma_min'
            
            # Cr√©er le scheduler avec la config corrig√©e
            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                scheduler_config
            )
        except Exception as e:
            # Si erreur, utiliser le scheduler par d√©faut du mod√®le
            print(f"‚ö†Ô∏è Impossible de configurer DPM-Solver, utilisation du scheduler par d√©faut: {e}")
            print("üí° Le mod√®le utilisera son scheduler par d√©faut (g√©n√©ralement aussi efficace)")
        
        # Optimisations m√©moire
        if self.device == "cuda":
            self.pipe.enable_attention_slicing()
            # self.pipe.enable_xformers_memory_efficient_attention()  # Si xformers install√©
        else:
            # Optimisations pour CPU
            self.pipe.enable_attention_slicing(1)  # Attention slicing aussi sur CPU
            # CPU mode : utiliser float32 pour compatibilit√© et stabilit√©
            if self.dtype == torch.float16:
                print("‚ö†Ô∏è float16 sur CPU non recommand√©, passage √† float32 pour compatibilit√©")
                self.dtype = torch.float32
                # Recharger avec float32
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    settings.SD_MODEL_ID,
                    dtype=torch.float32,  # Utiliser dtype au lieu de torch_dtype (d√©pr√©ci√©)
                    safety_checker=None,
                    requires_safety_checker=False
                ).to(self.device)
                
                # Configurer le scheduler (g√©rer les configurations diff√©rentes)
                try:
                    scheduler_config = self.pipe.scheduler.config.copy()
                    if hasattr(scheduler_config, 'final_sigmas_type'):
                        if scheduler_config.get('final_sigmas_type') == 'zero' and \
                           scheduler_config.get('algorithm_type') == 'deis':
                            scheduler_config['final_sigmas_type'] = 'sigma_min'
                    self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                        scheduler_config
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Scheduler par d√©faut utilis√©: {e}")
                
                self.pipe.enable_attention_slicing(1)
    
    def generate(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        guidance_scale: float = 7.5,
        num_inference_steps: int = 50,
        width: int = 512,
        height: int = 512,
        seed: Optional[int] = None
    ) -> Image.Image:
        """
        G√©n√®re une image avec Stable Diffusion.
        
        Args:
            prompt: Prompt textuel
            negative_prompt: Prompt n√©gatif (optionnel)
            guidance_scale: Force d'adh√©sion au prompt
            num_inference_steps: Nombre d'√©tapes de d√©bruitage
            width: Largeur de l'image
            height: Hauteur de l'image
            seed: Seed pour reproductibilit√©
        
        Returns:
            PIL.Image: Image g√©n√©r√©e
        """
        # G√©n√©rateur avec seed
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
        
        # G√©n√©ration
        with torch.inference_mode():
            image = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                width=width,
                height=height,
                generator=generator
            ).images[0]
        
        return image

# Instance globale
sd_generator = StableDiffusionGenerator()

